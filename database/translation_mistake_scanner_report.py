import os
import json
import re
from collections import defaultdict
from pathlib import Path
from typing import List, Dict, Set, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib
from matplotlib.patches import Rectangle

# Configure matplotlib for Chinese characters
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

"""
==========================================
å­¦ç”Ÿç¿»è¯‘é”™è¯¯åˆ†æå·¥å…·
==========================================

åŠŸèƒ½è¯´æ˜ï¼š
1. è§£æå­¦ç”Ÿç¿»è¯‘ä½œä¸šçš„JSONæ—¥å¿—æ–‡ä»¶
2. æå–æ ‡è®°ä¸º"ç¿»å¾—ä¸å¥½"çš„é”™è¯¯
3. ç”Ÿæˆé”™è¯¯ç»Ÿè®¡æŠ¥å‘Š
4. å¯¼å‡ºä¸¤ä¸ªJSONæ–‡ä»¶ï¼š
   - 1_student_mistakes.json: æŒ‰ä¸­æ–‡å¥å­åˆ†ç»„çš„å­¦ç”Ÿé”™è¯¯
   - 2_statistics_summary.json: æ¯ä¸ªå¥å­çš„ç»Ÿè®¡æ‘˜è¦
5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼š
   - mistake_rate_pie_charts.png: æ¯å¥è¯çš„é”™è¯¯ç‡é¥¼å›¾
   - student_mistakes_visual.png: å­¦ç”Ÿé”™è¯¯è¯¦ç»†åˆ—è¡¨å›¾

ä½¿ç”¨æ–¹æ³•ï¼š
1. è®¾ç½® ROOT_DIRECTORY ä¸ºåŒ…å«æ‰€æœ‰å­¦ç”Ÿæ–‡ä»¶å¤¹çš„æ ¹ç›®å½•
2. è®¾ç½® BASELINE_FOLDER ä¸ºåŸºå‡†å­¦ç”Ÿçš„æ–‡ä»¶å¤¹åç§°ï¼ˆç”¨äºæå–é¢˜ç›®ï¼‰
3. è¿è¡Œè„šæœ¬ï¼Œè‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰æŠ¥å‘Šå’Œå›¾è¡¨

è¾“å‡ºæ–‡ä»¶ï¼š
- 1_student_mistakes.json: å­¦ç”Ÿé”™è¯¯è¯¦æƒ…
- 2_statistics_summary.json: ç»Ÿè®¡æ‘˜è¦
- mistake_rate_pie_charts.png: é”™è¯¯ç‡é¥¼å›¾
- student_mistakes_visual.png: é”™è¯¯è¯¦æƒ…å¯è§†åŒ–å›¾
"""

# ==========================================
# æ•°æ®æ¨¡å‹ (ç”¨äºç±»å‹å®‰å…¨)
# ==========================================

@dataclass
class MistakeEntry:
    """å•ä¸ªé”™è¯¯æ¡ç›®çš„æ•°æ®æ¨¡å‹"""
    chinese_txt: str
    mistake: str
    mistake_flag: str
    comment: str
    std_input: str
    thought: str
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'MistakeEntry':
        """ä»å­—å…¸åˆ›å»º MistakeEntry å¯¹è±¡"""
        return cls(
            chinese_txt=data.get('chinese_txt', ''),
            mistake=data.get('mistake', ''),
            mistake_flag=data.get('mistake_flag', ''),
            comment=data.get('comment', ''),
            std_input=data.get('std_input', ''),
            thought=data.get('thought', '')
        )

@dataclass
class StudentMistake:
    """å­¦ç”Ÿç‰¹å®šé”™è¯¯çš„æ•°æ®æ¨¡å‹"""
    student_name: str
    mistake: str
    comment: str
    std_input: str
    file_path: str

# ==========================================
# ç¬¬ä¸€å±‚: æ–‡ä»¶è¯»å†™ä¸è§£æ (åº•å±‚)
# ==========================================

def parse_log_content(file_path: Path) -> Optional[List[Dict[str, Any]]]:
    """
    è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæå–å¹¶åŠ è½½å†…éƒ¨JSONæ•°æ®
    æ­£ç¡®å¤„ç†è½¬ä¹‰çš„å¼•å·å’Œæ’‡å·
    
    å‚æ•°ï¼š
        file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
    è¿”å›ï¼š
        è§£æåçš„JSONæ•°æ®åˆ—è¡¨ï¼Œå¤±è´¥åˆ™è¿”å›None
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()
        
        # é¦–å…ˆåŠ è½½å¤–å±‚JSONç»“æ„
        outer_data = json.loads(raw_content)
        
        # å¯¼èˆªåˆ° raw_content å­—æ®µ
        if 'raw_messages' not in outer_data or len(outer_data['raw_messages']) == 0:
            return None
        
        raw_message = outer_data['raw_messages'][0]['raw_content']
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– content='...' éƒ¨åˆ†
        match = re.search(r"content='(\{.*\})'", raw_message, re.DOTALL)
        if not match:
            return None
        
        json_string = match.group(1)
        
        # æ›¿æ¢æœ‰é—®é¢˜çš„è½¬ä¹‰åºåˆ—
        cleaned_string = json_string.replace("\\'", "'")
        
        # è§£ææ¸…ç†åçš„JSON
        data = json.loads(cleaned_string)
        
        return data.get("output_arr_obj")

    except json.JSONDecodeError as e:
        print(f"âš  JSONè§£æé”™è¯¯: {file_path.name}")
        return None
    except FileNotFoundError:
        print(f"âš  æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return None
    except Exception as e:
        print(f"âš  å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {file_path.name}: {e}")
        return None

def find_json_files(root_folder: Path) -> List[Path]:
    """
    é€’å½’æŸ¥æ‰¾ç»™å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰.jsonæ–‡ä»¶
    
    å‚æ•°ï¼š
        root_folder: è¦æœç´¢çš„æ ¹ç›®å½•
        
    è¿”å›ï¼š
        æŒ‡å‘JSONæ–‡ä»¶çš„Pathå¯¹è±¡åˆ—è¡¨
    """
    return list(root_folder.rglob("*.json"))

# ==========================================
# ç¬¬äºŒå±‚: æ•°æ®æå– (ä¸šåŠ¡é€»è¾‘)
# ==========================================

def extract_mistakes_from_data(
    parsed_data: List[Dict[str, Any]], 
    target_flag: str = "ç¿»å¾—ä¸å¥½"
) -> List[MistakeEntry]:
    """
    æå–æ ‡å¿—åŒ¹é…ç›®æ ‡çš„é”™è¯¯
    
    å‚æ•°ï¼š
        parsed_data: ä»æ—¥å¿—æ–‡ä»¶è§£æçš„JSONæ•°æ®
        target_flag: è¦è¿‡æ»¤çš„é”™è¯¯æ ‡å¿—ï¼ˆé»˜è®¤ï¼š"ç¿»å¾—ä¸å¥½"ï¼‰
        
    è¿”å›ï¼š
        åŒ¹é…ç›®æ ‡æ ‡å¿—çš„ MistakeEntry å¯¹è±¡åˆ—è¡¨
    """
    mistakes = []
    if not parsed_data:
        return mistakes
        
    for item in parsed_data:
        if item.get("mistake_flag") == target_flag:
            try:
                mistakes.append(MistakeEntry.from_dict(item))
            except Exception as e:
                print(f"è­¦å‘Š: è§£æé”™è¯¯æ¡ç›®å¤±è´¥: {e}")
    
    return mistakes

def extract_all_chinese_sentences(parsed_data: List[Dict[str, Any]]) -> Set[str]:
    """
    ä»è§£æçš„æ•°æ®ä¸­æå–æ‰€æœ‰å”¯ä¸€çš„ä¸­æ–‡å¥å­
    
    å‚æ•°ï¼š
        parsed_data: ä»æ—¥å¿—æ–‡ä»¶è§£æçš„JSONæ•°æ®
        
    è¿”å›ï¼š
        å”¯ä¸€ä¸­æ–‡å¥å­çš„é›†åˆ
    """
    sentences = set()
    if not parsed_data:
        return sentences
    
    for item in parsed_data:
        chinese_txt = item.get('chinese_txt')
        if chinese_txt:
            sentences.add(chinese_txt.strip())
    
    return sentences

# ==========================================
# ç¬¬ä¸‰å±‚: åŸºå‡†ç®¡ç†
# ==========================================

def establish_baseline_sentences(baseline_folder_path: Path) -> Set[str]:
    """
    ä»åŸºå‡†æ–‡ä»¶å¤¹ä¸­æå–å”¯ä¸€çš„ä¸­æ–‡å¥å­
    è¿™å°†åˆ›å»ºä¸€ä¸ªå‚è€ƒé›†ï¼Œç”¨äºåŒ¹é…å…¶ä»–å­¦ç”Ÿçš„ä½œä¸š
    
    å‚æ•°ï¼š
        baseline_folder_path: åŸºå‡†æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼ˆç¬¬ä¸€ä¸ªå­¦ç”Ÿæ–‡ä»¶å¤¹ï¼‰
        
    è¿”å›ï¼š
        ä½œä¸ºåŸºå‡†çš„å”¯ä¸€ä¸­æ–‡å¥å­é›†åˆ
    """
    baseline_sentences = set()
    json_files = find_json_files(baseline_folder_path)
    
    for file_path in json_files:
        parsed_data = parse_log_content(file_path)
        if parsed_data:
            sentences = extract_all_chinese_sentences(parsed_data)
            baseline_sentences.update(sentences)
    
    print(f"âœ“ åŸºå‡†å·²å»ºç«‹: ä» {baseline_folder_path.name} æå–äº† {len(baseline_sentences)} ä¸ªå¥å­")
    return baseline_sentences

# ==========================================
# ç¬¬å››å±‚: é”™è¯¯æ±‡æ€»ä¸ç»Ÿè®¡
# ==========================================

def summarize_student_mistakes(
    root_directory: str, 
    baseline_folder_name: str
) -> Tuple[Dict[str, List[StudentMistake]], Set[str]]:
    """
    åè°ƒæŸ¥æ‰¾ã€åŒ¹é…å’Œæ±‡æ€»é”™è¯¯çš„è¿‡ç¨‹
    
    å‚æ•°ï¼š
        root_directory: åŒ…å«æ‰€æœ‰å­¦ç”Ÿæ–‡ä»¶å¤¹çš„æ ¹è·¯å¾„
        baseline_folder_name: ç”¨ä½œåŸºå‡†çš„æ–‡ä»¶å¤¹åç§°
        
    è¿”å›ï¼š
        å…ƒç»„ (mistake_summary, baseline_sentences)
        - mistake_summary: å°† chinese_txt æ˜ å°„åˆ° StudentMistake å¯¹è±¡åˆ—è¡¨çš„å­—å…¸
        - baseline_sentences: åŸºå‡†ä¸­æ–‡å¥å­é›†åˆ
    """
    root_path = Path(root_directory)
    baseline_path = root_path / baseline_folder_name
    
    # éªŒè¯åŸºå‡†æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not baseline_path.is_dir():
        raise FileNotFoundError(
            f"åŸºå‡†æ–‡ä»¶å¤¹ '{baseline_folder_name}' æœªåœ¨ '{root_directory}' ä¸­æ‰¾åˆ°"
        )

    # æ­¥éª¤1: ä»ç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹å»ºç«‹åŸºå‡†
    baseline_sentences = establish_baseline_sentences(baseline_path)
    
    # æ­¥éª¤2: å¤„ç†æ‰€æœ‰å­¦ç”Ÿæ–‡ä»¶å¤¹
    mistake_summary = defaultdict(list)
    student_count = 0
    
    for student_folder_path in sorted(root_path.iterdir()):
        # è·³è¿‡éç›®å½•å’ŒåŸºå‡†æ–‡ä»¶å¤¹
        if not student_folder_path.is_dir():
            continue
            
        student_name = student_folder_path.name
        student_count += 1
        
        # æŸ¥æ‰¾å¹¶å¤„ç†è¯¥å­¦ç”Ÿçš„æ‰€æœ‰JSONæ–‡ä»¶
        student_json_files = find_json_files(student_folder_path)
        
        mistakes_count = 0
        for file_path in student_json_files:
            parsed_data = parse_log_content(file_path)
            mistakes_found = extract_mistakes_from_data(parsed_data)
            
            for mistake_entry in mistakes_found:
                sentence = mistake_entry.chinese_txt.strip()
                
                # åªè®°å½•åœ¨åŸºå‡†ä¸­çš„å¥å­
                if sentence in baseline_sentences:
                    mistake_summary[sentence].append(StudentMistake(
                        student_name=student_name,
                        mistake=mistake_entry.mistake,
                        comment=mistake_entry.comment,
                        std_input=mistake_entry.std_input,
                        file_path=str(file_path.name)
                    ))
                    mistakes_count += 1
    
    print(f"âœ“ å·²å¤„ç† {student_count} åå­¦ç”Ÿ")
    return dict(mistake_summary), baseline_sentences

# ==========================================
# ç¬¬äº”å±‚: ç»Ÿè®¡ä¸æŠ¥å‘Š
# ==========================================

def generate_statistics_report(
    mistake_summary: Dict[str, List[StudentMistake]]
) -> Dict[str, Any]:
    """
    ä»é”™è¯¯æ±‡æ€»ä¸­ç”Ÿæˆç»¼åˆç»Ÿè®¡ä¿¡æ¯
    
    å‚æ•°ï¼š
        mistake_summary: å°† chinese_txt æ˜ å°„åˆ°å­¦ç”Ÿé”™è¯¯çš„å­—å…¸
        
    è¿”å›ï¼š
        åŒ…å«å„ç§ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    stats = {
        "total_unique_sentences": len(mistake_summary),
        "total_mistake_instances": sum(len(students) for students in mistake_summary.values()),
        "mistakes_per_student": defaultdict(int),
        "sentences_with_most_mistakes": [],
        "students_processed": set()
    }
    
    # ç»Ÿè®¡æ¯ä¸ªå­¦ç”Ÿçš„é”™è¯¯æ•°é‡
    for sentence, student_mistakes in mistake_summary.items():
        for student_mistake in student_mistakes:
            stats["mistakes_per_student"][student_mistake.student_name] += 1
            stats["students_processed"].add(student_mistake.student_name)
    
    # æŒ‰é”™è¯¯é¢‘ç‡æ’åºå¥å­
    sentence_freq = [
        (sentence, len(students)) 
        for sentence, students in mistake_summary.items()
    ]
    stats["sentences_with_most_mistakes"] = sorted(
        sentence_freq, 
        key=lambda x: x[1], 
        reverse=True
    )
    
    # å°†é›†åˆè½¬æ¢ä¸ºè®¡æ•°
    stats["total_students"] = len(stats["students_processed"])
    del stats["students_processed"]
    
    return dict(stats)

def export_summary_to_json(
    mistake_summary: Dict[str, List[StudentMistake]], 
    output_path: str,
    include_metadata: bool = True
) -> None:
    """
    å°†é”™è¯¯æ±‡æ€»å¯¼å‡ºåˆ°JSONæ–‡ä»¶
    
    å‚æ•°ï¼š
        mistake_summary: è¦å¯¼å‡ºçš„é”™è¯¯å­—å…¸
        output_path: ä¿å­˜JSONæ–‡ä»¶çš„è·¯å¾„
        include_metadata: æ˜¯å¦åŒ…å«æ—¶é—´æˆ³ç­‰å…ƒæ•°æ®
    """
    export_data = {}
    
    if include_metadata:
        export_data["metadata"] = {
            "export_timestamp": datetime.now().isoformat(),
            "total_sentences": len(mistake_summary),
            "total_instances": sum(len(v) for v in mistake_summary.values())
        }
    
    export_data["mistakes"] = {
        sentence: [
            {
                "student": sm.student_name,
                "mistake": sm.mistake,
                "comment": sm.comment,
                "std_input": sm.std_input,
                "file": sm.file_path
            }
            for sm in students
        ]
        for sentence, students in mistake_summary.items()
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ å·²å¯¼å‡ºæ±‡æ€»åˆ°: {output_path}")

def export_student_mistakes_json(
    mistake_summary: Dict[str, List[StudentMistake]], 
    baseline_sentences: Set[str],
    output_path: str
) -> None:
    """
    å¯¼å‡ºæŒ‰å­¦ç”Ÿç»„ç»‡çš„æ¯ä¸ªä¸­æ–‡å¥å­çš„é”™è¯¯æ±‡æ€»
    
    æ ¼å¼:
    {
      "chinese_sentence": {
        "student_name": "mistake_text",
        ...
      },
      ...
    }
    
    å‚æ•°ï¼š
        mistake_summary: å°† chinese_txt æ˜ å°„åˆ°å­¦ç”Ÿé”™è¯¯çš„å­—å…¸
        baseline_sentences: æ‰€æœ‰åŸºå‡†å¥å­çš„é›†åˆ
        output_path: ä¿å­˜JSONæ–‡ä»¶çš„è·¯å¾„
    """
    export_data = {}
    
    # å¤„ç†åŸºå‡†ä¸­çš„æ¯ä¸ªå¥å­
    for sentence in sorted(baseline_sentences):
        student_mistakes_dict = {}
        
        # è·å–è¯¥å¥å­çš„æ‰€æœ‰é”™è¯¯
        if sentence in mistake_summary:
            for student_mistake in mistake_summary[sentence]:
                student_mistakes_dict[student_mistake.student_name] = student_mistake.mistake
        
        # åªåŒ…å«æœ‰é”™è¯¯çš„å¥å­
        if student_mistakes_dict:
            export_data[sentence] = student_mistakes_dict
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ å·²å¯¼å‡ºå­¦ç”Ÿé”™è¯¯åˆ°: {output_path}")


def export_statistics_json(
    mistake_summary: Dict[str, List[StudentMistake]], 
    baseline_sentences: Set[str],
    total_students: int,
    output_path: str
) -> None:
    """
    å¯¼å‡ºæ¯ä¸ªä¸­æ–‡å¥å­çš„ç»Ÿè®¡æ‘˜è¦ï¼ˆä¸åŒ…å«å­¦ç”Ÿå§“åï¼‰
    
    æ ¼å¼:
    {
      "chinese_sentence": {
        "total_submissions": 10,
        "mistake_count": 3,
        "mistake_rate": "30.00%",
        "unique_mistakes": ["mistake1", "mistake2", ...]
      },
      ...
    }
    
    å‚æ•°ï¼š
        mistake_summary: å°† chinese_txt æ˜ å°„åˆ°å­¦ç”Ÿé”™è¯¯çš„å­—å…¸
        baseline_sentences: æ‰€æœ‰åŸºå‡†å¥å­çš„é›†åˆ
        total_students: å¤„ç†çš„å­¦ç”Ÿæ€»æ•°
        output_path: ä¿å­˜JSONæ–‡ä»¶çš„è·¯å¾„
    """
    export_data = {}
    
    # å¤„ç†åŸºå‡†ä¸­çš„æ¯ä¸ªå¥å­
    for sentence in sorted(baseline_sentences):
        # æ”¶é›†è¯¥å¥å­çš„æ‰€æœ‰å”¯ä¸€é”™è¯¯ï¼ˆä¸åŒ…å«å­¦ç”Ÿå§“åï¼‰
        unique_mistakes = set()
        mistake_count = 0
        
        if sentence in mistake_summary:
            mistake_count = len(mistake_summary[sentence])
            for student_mistake in mistake_summary[sentence]:
                if student_mistake.mistake:  # åªæ·»åŠ éç©ºé”™è¯¯
                    unique_mistakes.add(student_mistake.mistake)
        
        # è®¡ç®—é”™è¯¯ç‡
        mistake_rate = (mistake_count / total_students * 100) if total_students > 0 else 0
        
        export_data[sentence] = {
            "total_submissions": total_students,
            "mistake_count": mistake_count,
            "mistake_rate": f"{mistake_rate:.2f}%",
            "unique_mistakes": sorted(list(unique_mistakes))  # æ’åºä»¥ä¿æŒä¸€è‡´æ€§
        }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ å·²å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ°: {output_path}")


def create_pie_charts_from_json(json_path: str, output_folder: str) -> None:
    """
    ä»ç»Ÿè®¡æ‘˜è¦JSONæ–‡ä»¶åˆ›å»ºé¥¼å›¾
    
    å‚æ•°ï¼š
        json_path: 2_statistics_summary.json æ–‡ä»¶çš„è·¯å¾„
        output_folder: ä¿å­˜è¾“å‡ºå›¾è¡¨çš„æ–‡ä»¶å¤¹
    """
    # åŠ è½½JSONæ•°æ®
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if not data:
        print("æ²¡æœ‰æ•°æ®å¯ç»˜åˆ¶!")
        return
    
    num_sentences = len(data)
    
    # è®¡ç®—ç½‘æ ¼å¤§å°
    cols = 3
    rows = (num_sentences + cols - 1) // cols  # å‘ä¸Šå–æ•´
    
    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
    fig.suptitle('å„å¥ç¿»è¯‘é”™è¯¯ç‡', fontsize=16, fontweight='bold')
    
    # å±•å¹³axesä»¥ä¾¿äºè¿­ä»£ï¼ˆå¤„ç†å•è¡Œæƒ…å†µï¼‰
    if num_sentences == 1:
        axes_flat = [axes]
    elif rows == 1:
        axes_flat = axes
    else:
        axes_flat = axes.flatten()
    
    # é…è‰²æ–¹æ¡ˆ
    colors = ['#66c2a5', '#fc8d62']  # ç»¿è‰²è¡¨ç¤ºæ­£ç¡®ï¼Œæ©™è‰²è¡¨ç¤ºé”™è¯¯
    explode = (0.05, 0)  # ç¨å¾®åˆ†ç¦»é”™è¯¯åˆ‡ç‰‡
    
    for idx, (sentence, stats) in enumerate(data.items()):
        ax = axes_flat[idx]
        
        # è®¡ç®—æ­£ç¡®å’Œé”™è¯¯çš„æ•°é‡
        total = stats['total_submissions']
        incorrect = stats['mistake_count']
        correct = total - incorrect
        
        # é¥¼å›¾æ•°æ®
        sizes = [correct, incorrect]
        labels = [f'æ­£ç¡®\n({correct}/{total})', f'é”™è¯¯\n({incorrect}/{total})']
        
        # åˆ›å»ºé¥¼å›¾
        wedges, texts, autotexts = ax.pie(
            sizes, 
            labels=labels, 
            colors=colors,
            autopct='%1.0f%%',
            startangle=90,
            explode=explode if incorrect > 0 else (0, 0),
            textprops={'fontsize': 10}
        )
        
        # åŠ ç²—ç™¾åˆ†æ¯”æ–‡æœ¬
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(11)
        
        # æ·»åŠ å¸¦å¥å­çš„æ ‡é¢˜ï¼ˆå¦‚æœå¤ªé•¿åˆ™æˆªæ–­ï¼‰
        sentence_short = sentence[:35] + '...' if len(sentence) > 35 else sentence
        ax.set_title(f"{idx+1}. {sentence_short}", fontsize=11, pad=10, wrap=True)
    
    # éšè—æœªä½¿ç”¨çš„å­å›¾
    for idx in range(num_sentences, len(axes_flat)):
        axes_flat[idx].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸­
    output_path = os.path.join(output_folder, 'mistake_rate_pie_charts.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ é¥¼å›¾å·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¸è‡ªåŠ¨æ˜¾ç¤ºå›¾è¡¨ä»¥é¿å…é˜»å¡
    # plt.show()


def create_student_mistakes_visual(json_path: str, output_folder: str) -> None:
    """
    ä» 1_student_mistakes.json åˆ›å»ºå¯è§†åŒ–å›¾ç‰‡
    æ˜¾ç¤ºä¸­æ–‡å¥å­åŠå…¶å¯¹åº”çš„å­¦ç”Ÿé”™è¯¯
    
    å‚æ•°ï¼š
        json_path: 1_student_mistakes.json æ–‡ä»¶çš„è·¯å¾„
        output_folder: ä¿å­˜è¾“å‡ºå›¾ç‰‡çš„æ–‡ä»¶å¤¹
    """
    # åŠ è½½JSONæ•°æ®
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if not data:
        print("æ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º!")
        return
    
    # è®¡ç®—éœ€è¦çš„å›¾ç‰‡é«˜åº¦
    num_sentences = len(data)
    
    # åŠ¨æ€è®¡ç®—é«˜åº¦ï¼šæ¯ä¸ªå¥å­åŒºå—çº¦å  1.5 è‹±å¯¸
    fig_height = max(8, num_sentences * 1.5)
    
    fig, ax = plt.subplots(figsize=(14, fig_height))
    ax.axis('off')
    
    # è®¾ç½®æ ‡é¢˜
    title_text = 'å­¦ç”Ÿç¿»è¯‘é”™è¯¯è¯¦æƒ…'
    fig.suptitle(title_text, fontsize=18, fontweight='bold', y=0.98)
    
    # èµ·å§‹yåæ ‡
    y_position = 0.95
    x_left = 0.05
    line_height = 0.85 / num_sentences  # æ ¹æ®å¥å­æ•°é‡åŠ¨æ€è°ƒæ•´è¡Œé«˜
    
    # é¢œè‰²é…ç½®
    sentence_color = '#2c3e50'  # æ·±ç°è“è‰² - ä¸­æ–‡å¥å­
    student_color = '#e74c3c'   # çº¢è‰² - å­¦ç”Ÿåå­—
    mistake_color = '#34495e'   # æ·±ç°è‰² - é”™è¯¯å†…å®¹
    box_color = '#ecf0f1'       # æµ…ç°è‰² - èƒŒæ™¯æ¡†
    
    for idx, (sentence, student_mistakes) in enumerate(data.items()):
        # ç»˜åˆ¶èƒŒæ™¯æ¡†
        if idx % 2 == 0:
            rect = Rectangle((x_left - 0.01, y_position - line_height + 0.01), 
                           0.92, line_height - 0.01, 
                           facecolor=box_color, edgecolor='none', 
                           transform=fig.transFigure, zorder=1)
            fig.patches.append(rect)
        
        # 1. æ˜¾ç¤ºä¸­æ–‡å¥å­ï¼ˆåŠ ç²—ï¼‰
        sentence_display = f"{idx + 1}. {sentence}"
        fig.text(x_left, y_position, sentence_display, 
                fontsize=11, fontweight='bold', color=sentence_color,
                va='top', ha='left', wrap=True, transform=fig.transFigure, zorder=2)
        
        y_position -= line_height * 0.35
        
        # 2. æ˜¾ç¤ºå­¦ç”Ÿé”™è¯¯
        for student_name, mistake_text in student_mistakes.items():
            mistake_line = f"   â€¢ {student_name}: {mistake_text}"
            fig.text(x_left + 0.02, y_position, mistake_line,
                    fontsize=9, color=mistake_color,
                    va='top', ha='left', wrap=True, transform=fig.transFigure, zorder=2)
            y_position -= line_height * 0.25
        
        # å¥å­ä¹‹é—´çš„é—´è·
        y_position -= line_height * 0.15
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    output_path = os.path.join(output_folder, 'student_mistakes_visual.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ“ å­¦ç”Ÿé”™è¯¯å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {output_path}")
    
    plt.close()


# ==========================================
# ä¸»ç¨‹åºæ‰§è¡Œ
# ==========================================

if __name__ == '__main__':
    # é…ç½®
    ROOT_DIRECTORY = r"E:\zhenzhen_eng_coze\example\é«˜ä¸‰_9_reduced"
    BASELINE_FOLDER = "ä¹”å­æ´‹"
    OUTPUT_JSON_STUDENTS = os.path.join(ROOT_DIRECTORY, "1_student_mistakes.json")
    OUTPUT_JSON_STATISTICS = os.path.join(ROOT_DIRECTORY, "2_statistics_summary.json")

    try:
        print("=" * 60)
        print("å­¦ç”Ÿç¿»è¯‘é”™è¯¯åˆ†æ")
        print("=" * 60)
        
        # è¿è¡Œæ±‡æ€»è¿‡ç¨‹
        final_summary, baseline_sentences = summarize_student_mistakes(
            ROOT_DIRECTORY, 
            BASELINE_FOLDER
        )
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = generate_statistics_report(final_summary)
        
        # ç®€å•æ±‡æ€»è¾“å‡º
        print(f"\nğŸ“Š æ±‡æ€»:")
        print(f"   â€¢ æœ‰é”™è¯¯çš„å¥å­æ•°: {stats['total_unique_sentences']}")
        print(f"   â€¢ é”™è¯¯å®ä¾‹æ€»æ•°: {stats['total_mistake_instances']}")
        print(f"   â€¢ å·²å¤„ç†å­¦ç”Ÿæ•°: {stats['total_students']}")
        
        # å¯¼å‡ºJSONæ–‡ä»¶
        print(f"\nğŸ“ å¯¼å‡ºæ–‡ä»¶...")
        export_student_mistakes_json(
            final_summary, 
            baseline_sentences,
            OUTPUT_JSON_STUDENTS
        )
        
        export_statistics_json(
            final_summary, 
            baseline_sentences,
            stats['total_students'],
            OUTPUT_JSON_STATISTICS
        )
        
        # åˆ›å»ºé¥¼å›¾
        print(f"\nğŸ“ˆ ç”Ÿæˆé¥¼å›¾...")
        create_pie_charts_from_json(OUTPUT_JSON_STATISTICS, ROOT_DIRECTORY)
        
        # åˆ›å»ºå­¦ç”Ÿé”™è¯¯å¯è§†åŒ–å›¾
        print(f"\nğŸ“ˆ ç”Ÿæˆå­¦ç”Ÿé”™è¯¯è¯¦æƒ…å›¾...")
        create_student_mistakes_visual(OUTPUT_JSON_STUDENTS, ROOT_DIRECTORY)
        
        print(f"\nâœ… æ‰€æœ‰ä»»åŠ¡å·²æˆåŠŸå®Œæˆ!")
            
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
    except Exception as e:
        print(f"\nâŒ æ„å¤–é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()