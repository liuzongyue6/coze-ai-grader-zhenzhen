"""
é€šç”¨çš„apiå¤„ç†å™¨
åå¤„ç†ç¼“å­˜æ–‡ä»¶ - è§£æåŸå§‹APIå“åº”å¹¶ç”Ÿæˆæ ¼å¼åŒ–çš„æ‰¹æ”¹ç»“æœ
æ”¯æŒå•è¾“å‡ºå’Œå¤šè¾“å‡ºæ ¼å¼é…ç½®
"""

import os
import json
import glob
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union

def extract_json_from_content(content_str: str, content_pattern: str) -> Optional[Dict]:
    """ä»contentå­—ç¬¦ä¸²ä¸­æå–JSONæ•°æ®"""
    try:
        content_match = re.search(content_pattern, content_str, re.DOTALL)
        
        if content_match:
            json_str = content_match.group(1)
            # å¤„ç†è½¬ä¹‰å­—ç¬¦
            json_str = json_str.replace("\\'", "'")
            return json.loads(json_str)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°content=æ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ
            return json.loads(content_str)
    except (json.JSONDecodeError, AttributeError) as e:
        print(f"JSONè§£æå¤±è´¥: {str(e)}")
        print(f"åŸå§‹å†…å®¹: {content_str[:200]}...")
        return None

def is_multi_output_config(config: Dict[str, Any]) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºå¤šè¾“å‡ºé…ç½®"""
    return "output_types" in config and "global_config" in config

def format_single_section(data_array: List[Dict], section_config: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å•ä¸ªæ•°æ®æ®µ"""
    formatted_text = ""
    
    # æ ‡é¢˜å’Œæ‘˜è¦
    title = section_config["title"]
    summary_template = section_config["summary_template"]
    formatted_text += f"{title}\n\n"
    formatted_text += f"{summary_template.format(count=len(data_array))}\n\n"
    
    # å¤„ç†æ¯ä¸ªé¡¹ç›®
    item_title_template = section_config["item_title_template"]
    separator = section_config["separator"]
    field_mappings = section_config["field_mappings"]
    field_order = section_config["field_order"]
    
    for idx, item in enumerate(data_array, 1):
        formatted_text += f"{item_title_template.format(index=idx)}\n"
        formatted_text += f"{separator}\n"
        
        # æŒ‰é…ç½®çš„é¡ºåºæ˜¾ç¤ºå­—æ®µ
        for field_name in field_order:
            if field_name in item and field_name in field_mappings:
                display_name = field_mappings[field_name]
                formatted_text += f"{display_name}:\n{item[field_name]}\n\n"
        
        formatted_text += f"{separator}\n\n"
    
    return formatted_text

def format_results_unified(parsed_data: Dict, config: Dict[str, Any]) -> str:
    """ç»Ÿä¸€æ ¼å¼åŒ–ç»“æœ - æ”¯æŒå•è¾“å‡ºå’Œå¤šè¾“å‡ºé…ç½®"""
    all_formatted_text = ""
    
    if is_multi_output_config(config):
        # å¤šè¾“å‡ºé…ç½®
        output_types = config["output_types"]
        sections_found = 0
        
        for output_type, output_config in output_types.items():
            if not output_config["enabled"]:
                continue
                
            if output_type not in parsed_data:
                print(f"âš ï¸ æ•°æ®ä¸­æœªæ‰¾åˆ°å­—æ®µ '{output_type}'ï¼Œè·³è¿‡")
                continue
            
            data_array = parsed_data[output_type]
            if not data_array:
                print(f"âš ï¸ å­—æ®µ '{output_type}' ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            if sections_found > 0:
                all_formatted_text += "\n" + "="*80 + "\n\n"
            
            formatted_section = format_single_section(data_array, output_config)
            all_formatted_text += formatted_section
            sections_found += 1
            
    else:
        # å•è¾“å‡ºé…ç½®
        data_field = config["data_field"]
        
        if data_field not in parsed_data:
            return f"æœªè¯†åˆ«çš„æ•°æ®æ ¼å¼ï¼Œæ‰¾ä¸åˆ°å­—æ®µ '{data_field}': {json.dumps(parsed_data, ensure_ascii=False, indent=2)}"
        
        data_array = parsed_data[data_field]
        all_formatted_text = format_single_section(data_array, config)
    
    return all_formatted_text

def get_file_header_template(config: Dict[str, Any]) -> str:
    """è·å–æ–‡ä»¶å¤´æ¨¡æ¿"""
    if is_multi_output_config(config):
        # å¤šè¾“å‡ºé…ç½® - ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„è¾“å‡ºç±»å‹çš„æ¨¡æ¿
        for output_type, output_config in config["output_types"].items():
            if output_config["enabled"]:
                return output_config["file_header_template"]
        return "=== å¤„ç†ç»“æœ ===\n\nå¤„ç†æ—¶é—´: {process_time}\nå­¦ç”Ÿå§“å: {folder_name}\nåŸå§‹ç¼“å­˜: {cache_file}\næ¶ˆæ¯æ•°é‡: {message_count}\n\n"
    else:
        # å•è¾“å‡ºé…ç½®
        return config["file_header_template"]

def get_output_prefix(config: Dict[str, Any]) -> str:
    """è·å–è¾“å‡ºæ–‡ä»¶å‰ç¼€"""
    if is_multi_output_config(config):
        return config["global_config"]["output_prefix"]
    else:
        return config["output_prefix"]

def process_cache_file(cache_file_path: str, config: Dict[str, Any]) -> bool:
    """å¤„ç†å•ä¸ªç¼“å­˜æ–‡ä»¶"""
    try:
        print(f"ğŸ“ å¤„ç†ç¼“å­˜æ–‡ä»¶: {os.path.basename(cache_file_path)}")
        
        # è¯»å–ç¼“å­˜æ–‡ä»¶
        with open(cache_file_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        folder_name = cache_data.get("folder_name", "Unknown")
        timestamp = cache_data.get("timestamp", datetime.now().strftime('%Y%m%d_%H%M%S'))
        messages = cache_data.get("raw_messages", [])
        
        if not messages:
            print(f"   âŒ ç¼“å­˜æ–‡ä»¶ä¸­æ²¡æœ‰æ¶ˆæ¯æ•°æ®")
            return False
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        cache_dir = os.path.dirname(cache_file_path)
        output_prefix = get_output_prefix(config)
        output_file = os.path.join(cache_dir, f"{folder_name}_{output_prefix}_{timestamp}.txt")
        
        all_formatted_results = []
        
        # å¤„ç†æ¯ä¸ªæ¶ˆæ¯
        for msg_data in messages:
            raw_content = msg_data.get("raw_content", "")
            
            # æå–å¹¶è§£æJSON
            content_pattern = config["content_pattern"]
            parsed_data = extract_json_from_content(raw_content, content_pattern)
            if parsed_data:
                formatted_result = format_results_unified(parsed_data, config)
                all_formatted_results.append(formatted_result)
        
        # å†™å…¥æ ¼å¼åŒ–ç»“æœåˆ°å•ä¸ªæ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            # ä½¿ç”¨é…ç½®çš„æ–‡ä»¶å¤´æ¨¡æ¿
            header_template = get_file_header_template(config)
            header = header_template.format(
                process_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                folder_name=folder_name,
                cache_file=os.path.basename(cache_file_path),
                message_count=len(messages)
            )
            f.write(header)
            
            for i, formatted_result in enumerate(all_formatted_results, 1):
                if len(all_formatted_results) > 1:
                    f.write(f"=== æ¶ˆæ¯ {i} å¤„ç†ç»“æœ ===\n\n")
                f.write(formatted_result)
                f.write("\n")
        
        print(f"   âœ… å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {os.path.basename(output_file)}")
        return True
        
    except Exception as e:
        print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")
        return False

def scan_and_process_cache_files(directory: str, config: Dict[str, Any]):
    """æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰ç¼“å­˜æ–‡ä»¶å¹¶å¤„ç†"""
    print(f"ğŸ” æ‰«æç›®å½•: {directory}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
    cache_pattern = os.path.join(directory, "**/*_response_cache_*.json")
    cache_files = glob.glob(cache_pattern, recursive=True)
    
    if not cache_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ–‡ä»¶")
        return
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
    
    success_count = 0
    for cache_file in cache_files:
        if process_cache_file(cache_file, config):
            success_count += 1
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆ! æˆåŠŸ: {success_count}/{len(cache_files)}")

def load_config() -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶ - è‡ªåŠ¨æ£€æµ‹é…ç½®ç±»å‹"""
    config_attempts = [
        ("config.translation_rec_format_config", "MULTI_OUTPUT_FORMAT_CONFIG", "å¤šè¾“å‡ºæ ¼å¼é…ç½®"),
        ("config.translation_format_config", "DEFAULT_FORMAT_CONFIG", "å•è¾“å‡ºæ ¼å¼é…ç½®")
    ]
    
    for module_name, config_name, desc in config_attempts:
        try:
            module = __import__(module_name, fromlist=[config_name])
            config = getattr(module, config_name)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {module_name}.py ({desc})")
            return config
        except (ImportError, AttributeError) as e:
            print(f"âš ï¸ å°è¯•åŠ è½½ {module_name} å¤±è´¥: {str(e)}")
            continue
    
    raise ImportError("æ— æ³•åŠ è½½ä»»ä½•é…ç½®æ–‡ä»¶ã€‚è¯·ç¡®ä¿å­˜åœ¨ä»¥ä¸‹é…ç½®æ–‡ä»¶ä¹‹ä¸€ï¼š\n"
                     "- config/translation_rec_format_config.py (å¤šè¾“å‡º)\n"
                     "- config/translation_format_config.py (å•è¾“å‡º)")

def main():
    """ä¸»å‡½æ•°"""
    print("=== ç¼“å­˜æ–‡ä»¶åå¤„ç†å™¨ ===\n")
    
    # åŠ è½½æ ¼å¼åŒ–é…ç½®
    try:
        config = load_config()
    except ImportError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    if is_multi_output_config(config):
        enabled_types = [k for k, v in config['output_types'].items() if v['enabled']]
        print(f"ğŸ“‹ é…ç½®ç±»å‹: å¤šè¾“å‡ºæ ¼å¼")
        print(f"ğŸ“‹ å¯ç”¨çš„è¾“å‡ºç±»å‹: {enabled_types}")
        print(f"ğŸ“‹ è¾“å‡ºå‰ç¼€: {config['global_config']['output_prefix']}\n")
    else:
        print(f"ğŸ“‹ é…ç½®ç±»å‹: å•è¾“å‡ºæ ¼å¼")
        print(f"ğŸ“‹ æ•°æ®å­—æ®µ: {config['data_field']}")
        print(f"ğŸ“‹ è¾“å‡ºå‰ç¼€: {config['output_prefix']}\n")
    
    # é…ç½®ç›®å½•
    test_directory = r"E:\zhenzhen_eng_coze\example\é«˜ä¸‰ç¬¬3å‘¨ä½œæ–‡_è¡¥_reduced"
    
    print(f"ğŸ“‚ æµ‹è¯•ç›®å½•: {test_directory}")
    
    if not os.path.exists(test_directory):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {test_directory}")
        return
    
    # å¤„ç†æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
    scan_and_process_cache_files(test_directory, config)

if __name__ == "__main__":
    main()
